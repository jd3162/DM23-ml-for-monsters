{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43fc350c",
   "metadata": {},
   "source": [
    "# Fine-tuning a HuggingFace model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef571f",
   "metadata": {},
   "source": [
    "## Code Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c9d5d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 16:37:53.742459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-03 16:37:54.566467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a42f35-8e80-4f9d-9453-0200b98ada93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (68.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.40.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.0)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8039c01",
   "metadata": {},
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "- A common use of LLMs is to leverage their **generalized** linguistic capacities by finetunint them for a **particular** task\n",
    "- For instance: We could take an LLM and train it to... \n",
    "    - classify text sequences\n",
    "    - classify tokens\n",
    "    - produce dialogue\n",
    "    - answer questions\n",
    "    - etc etc etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c9c5e1",
   "metadata": {},
   "source": [
    "## Author Attribution\n",
    "\n",
    "- The task I want to train the model to perform on is to identify authors of text\n",
    "    - This is known as \"author attribution\"\n",
    "    - E.g. Italian Computer Scientists tried to identify Elena Ferrante by comparing her work with known Italian authors and journalists\n",
    "- We'll be using one of the few author attribution datasets on Huggingface \n",
    "    - Uses text from 13 journalists at the Guardian\n",
    "-  We can find the [data here](https://huggingface.co/datasets/guardian_authorship)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ecc66",
   "metadata": {},
   "source": [
    "We load it by calling ```load_dataset```. The function needs the url of the dataset and a specification of which part of the data we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c161776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('guardian_authorship', 'cross_genre_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4854d0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 12,\n",
       " 'topic': 4,\n",
       " 'article': 'DBC Pierre, Booker Prize-winner and author of Vernon God Little, has moved to Ireland. The first rule of interviewing people in Ireland, before \"Wear thermal pants\" and \"Remember some euros\", is this: if they\\'re a writer, an artist or a composer, they\\'re there for the tax break. It irks me when these literary bad boys (especially Michel Houellebecq, who used to call himself a sodding communist) uproot themselves just, it seems fair to assume, to avoid a basic system of sharing stuff that your average apolitical cabbie seems able to manage. It irks me still further when they won\\'t admit it. \"No, no, no,\" says Dirty But Clean (that\\'s what his initials stand for). \"No. In the first instance, it was because I was in south London, and with the advance from Vernon I thought I should make something of it. I did, slowly, pay back my creditors. But I had to move somewhere I could afford to buy a place, and not fritter the rest of it in Soho.\" Not tax-related at all? \"No. Also, in Balham, the property boom spread from Clapham, and people started getting really uptight. The neighbours started getting really antsy.\" Apparently, they complained when he shuffled about in the night. But did you have any friends in Ireland? \"Well, no.\" The day he was leaving London was the first day they had parking restrictions on his street. \"I\\'d left the car with the front wheels over the line, and went off to sleep. I woke up, and of course the car was gone. They\\'d towed it and given it to the mafia that runs that car-towing scam, and I had to go down and pay a fuckload of money. So I got the car, and said, \\'I\\'m out of here.\\' Got to the place where I now live, put the car maybe half a kilometre up the road, and took a photograph of it, intending to send it to the car parking mafia, with a letter saying, \\'Come and get it now, you cunts.\\' Until finally I thought, \\'This is silly. Why am I being like this?\\' I really had city fever. I needed to get to the countryside. It\\'s odd, because I was raised in the city, but I needed to get out of it.\" I still don\\'t buy it, not a word, and I can\\'t bear the Tory implications of having a go at a car pound, but I\\'m really having to force myself to bring it up, because the truth is DBC Pierre is the most ludicrously charming individual. He is exactly as charming as you\\'d have to be to do people out of tens of thousands of pounds (as he once did) without recourse to a shotgun. He reminds me of that speech in Pulp Fiction where Samuel L Jackson explains why pigs are filthy and dogs aren\\'t. \"I wouldn\\'t go so far as to call a dog filthy, but they are definitely dirty. But a dog\\'s got personality; personality goes a long way.\" \"So, by that rationale, if a pig had a better personality, he would cease to be a filthy animal. Is that true?\" \"Well, we\\'d have to be talking one charming motherfucking pig.\" DBC might be dirty, in other words, but he\\'s too charming to be filthy. I know this isn\\'t brain fever brought on by the landscape (which, in Leitrim, where he now lives for un-tax-related reasons, is daunting and lovely): people like this guy. He\\'s sitting outside a shabby little bar with an unopened bottle of Martel. His face is pink with cold (you have to smoke outdoors in Ireland). I can\\'t work out whether this is his weekly shop, and it includes only brandy. Or whether he\\'s done the maths and figured that, for the amount of brandy he intends to drink right now, he might as well buy a whole bottle. Or whether it\\'s brandy from home, and he couldn\\'t leave it unattended because he had a delinquent staying with him. So it seems reasonable enough to ask, \"Why have you got that?\" \"They gave it to me.\" \"Why?\" \"It\\'s a present.\" \"For what?\" \"For Christmas.\" Never mind that it\\'s January - they must love him. When DBC Pierre won the Booker Prize in 2003, there was something dodgy about him on every conceivable level. The Daily Telegraph headlined his victory, \"Reformed cocaine addict is &pound;50,000 Booker winner.\" The Guardian reported, \"Repentant rogue wins over Booker Prize judges\" (more old-world charm to that, I feel). You can\\'t throw a stick without hitting an ex-junkie in the literary world, but Pierre was much more than just a former cocaine addict: a con artist and fraudster, he\\'d appeared in court in Australia charged with almost every minor felony connected with cash scams. Besides all that, the book had come out of nowhere, the guy had no literary credentials - even that felt like a bit of a scam. He wasn\\'t British, and wangled eligibility for the Booker on a technicality (he was born in Australia, in 1961, and grew up in Mexico). His name wasn\\'t even Pierre; his real name is Peter Finlay. Even his author photograph lies: in it, he looks round and red and slightly distant, like a city boy who already knows you don\\'t have enough money to be useful to him; in the flesh, he looks lean and enigmatic. Vernon God Little, Finlay/Pierre said at the time, and he confirms it now, was in many ways autobiographical. \"Erm ... Vernon was a lot about me,\" he says with a friendly smirk. \"It was a very easy sail to put up. I had lots of wind behind me.\" The story, naturally, wasn\\'t his story: his teen protagonist is best friend to a kid who goes loopy and kills his classmates, then himself. Vernon, scapegoat for a host of adult idiocies in the aftermath of the crime, copes with stuff and amuses himself by making things up. He lies, in the most abundant and devil-may-care manner, and ultimately gets himself into trouble as serious as, possibly even more serious than, if he\\'d been the killer himself. It\\'s a combination of his own mendacity and other people\\'s stupidity that finally does for Vernon, and I suppose that this is how Finlay sees his own life. \"Fact and fiction are a real problem for me. They\\'ve been a problem in my life, and they remain a problem still, in the way that we receive supposed facts, and the way that we distinguish them from fiction. You can watch whatever - pick any sitcom - one minute, and then the news, and then some bollocks, and then a documentary, and it\\'s an act of concentration to distinguish them. What obsessed me is that novelists spend a lot of time trying to make their fiction resonate and sound real - to give it gravity, to give it a sense of reality. And that really interested me, because I find reality as it moves very whimsical and puerile and very simplistic. I wanted to spend no time at all on issues like plausibility. It\\'s pointless to think, as you write, \\'Nobody\\'s going to believe that.\\' I watch the news and I don\\'t believe that. It\\'s horseshit. That\\'s so obviously a con or a lie.\" In conversation, and in his work, Finlay operates best when you don\\'t try to pin him down. In the past, as he says, he famously had a problem telling the truth. Now it\\'s more a problem for his audience - he\\'s put his fraudulent days behind him, paid back everyone to whom he owed money (with the exception of two people he can\\'t find), turned over a new leaf. Still, though, he\\'s slippery: when you try to unpick the narrative of his life, it is full of missing years, missing details, curious omissions. We know, for instance, that he grew up in Mexico City, where his parents\\' substantial wealth was magnified to a point of hyper-reality by the poverty of their environs. His father was a scientist, working on genetically modified crops, until he was diagnosed with a brain tumour; Finlay was 16. Both parents went to New York for his father\\'s treatment, leaving their son technically in the care of servants, but more accurately making 10 kinds of mayhem with his friends. He started taking drugs. When he was 19, his father died, bringing to a close his insane upbringing, which accounts for so much. It accounts for his crazy accent, which is totally consistent and totally impossible to locate in time and space - it\\'s more English than American, like a public schoolboy with a slight mid-European drawl. It accounts for his freakish relationship with money. \"Where I grew up, money came into the house in envelopes. You never went to the bank. The maids would get paid, I\\'d get my pocket money, that would be it. I came out of there with no sense of civic duty. In Mexico, you dealt with things as they came to your face. There was no law, and there was nothing you couldn\\'t get away with.\" In his early 20s, Finlay had his residency withdrawn (for bringing a foreign car over the border) and went first to Europe, then to Australia. \"So I came to Australia and Britain, which actually had laws and statutes and shit, and that\\'s what fucked me up. I came out of a melting pot into a bunch of cubicles. And it dismayed me. So you bank the cheque the day before the money\\'s in the account. How can that be a crime? Then the money doesn\\'t show up, so that becomes a crime. I spent a lot of my Australian visit in court. Literally every week.\" Whatever his creditors say, I believe him. I believe him when he says he never intended to scam anyone; most of his court appearances were for bank offences and bad cheques. I believe him when he says that he was constantly being offered personal bankruptcy and he never took it, because that would have been an admission that he\\'d never intended to pay these people back in the first place. Rackety characters such as Finlay, with his scrapes and his debts and his near-misses with incarceration, often sound rather romantic, and I assumed he might have some kind of attachment to this period of his life, but of course he doesn\\'t. \"It was a very painful time, incredibly painful. It kept me down for more than a decade.\" By the time he got to England in the early 1990s, he had been through the mill, and worked his only nine-to-five job, in an advertising agency. \"It didn\\'t last long. It was in the Caribbean. It was great, but then I would have had to live in the Caribbean for ever. You miss England, because it has a proper grounded feeling ... the smell of diesel in the air.\" Sure, I would miss England, but why would he, when, if he were to call anywhere home, it would be Mexico or Australia? (This is a tiny point, but it does crop up regularly, the sense that he\\'s describing himself in terms of the person he\\'s speaking to, using whatever information about them he has, however scant. That\\'s probably what it means to be a charmer, I suppose.) In London, Finlay started work on Vernon God Little (before that, the only thing he\\'d tried to write was a radio play). The fact that his adolescence was interrupted by his father\\'s death accounts, I think, for his very close rendition in Vernon God Little of how a teenager might think, and speak, and see things. I think it self-evident that the bereavement halted some part of his development, but he heads this off, rather gently. \"There\\'s a romantic answer to everything, where we\\'re prey to these energies, and there\\'s a psychological answer - you haven\\'t moved past this or that. And both have validity. But it\\'s not worth doing the chase. I\\'ve had therapy in the past, mainly because it was a requirement of court, and in the end I figured it was better not to understand, because that\\'s the engine that drives you.\" This mysterious engine has now driven him to write his second novel, Ludmila\\'s Broken English, which is about ... oh, it would be daft to tell you what it\\'s about. It does conjoined twins, and terrorism, and the sad, postwar limbo of the formerly Soviet Caucasus; it does sex and the absence of sex; it touches briefly on music and silence and short bursts of structurally incomprehensible violence - and it is great, and rollicking, and mischievously disrespectful to the literary establishment. Finlay has said before how charmed he always is by vernacular twists of speech. In Ludmila\\'s Broken English, this is more evident than ever: in the conjoined twins, separated at the start of the book, you can hear his fondness for English verbal quirks - \"You silly sausage\", \"bloody \\'ell\". But none of it sounds quite right, in so far as it doesn\\'t really sound English. Finlay doesn\\'t sound as if he\\'s closely observed or rendered this language, but as if he\\'s pinched what isn\\'t his, and now he\\'s bloody well going to enjoy it. When it comes to the Caucasians, he concocts the most explosively intricate and vivid register of scatological insult and imaginative excursions on the theme of brutality, most of it sexual. \"I\\'m really keen to be a good writer,\" he says. \"I thought I\\'d come away from the comfort zone of the first person, come away from the comfort zone of it being anything to do with me, write something that could have been written at any point over the past 100 years, just in terms of its structure, stand back and not get so emotionally involved with it.\" It\\'s true that the narrative engagement is less feverish than in Vernon, but one thing that strikes me is that, in deciding to move away from his own experience, Finlay produces something closer to himself. It\\'s great fun to read because he understands so instinctively what carries you from one page to the next; but what makes it so much more than just a page-turner is the force of his anti-authoritarianism. There\\'s no way this book could have been written \"at any point in the past 100 years\": structurally, it is crazily ill-disciplined, and takes a very modern delight in that. Using twins as his protagonists allows Finlay to have everything both ways: the two of them have all kinds of debates - on terrorism, on society - and Finlay never has to pin himself down. It often feels as if he is trying to decipher his own views through the conduit of twins, identical in every way, except that one is liberal, the other illiberal. \"Personally, on terrorism, as everything else, I\\'m probably very liberal at heart, but it\\'s unresolved. But I say this to you as a liar - I can tell better lies than any of these fuckers who get up on television and say they\\'re going to do this or that because of terrorism. They\\'re such flimsy and transparent people, they\\'re much greater psychotics than me.\" (To clarify, I had called his self-confidence psychotic a minute earlier. I didn\\'t mean to hurt his feelings, but think I may have done.) \"Greater in that they\\'re successful with it, we buy it - and that is the thing that has probably changed in me.\" He means his own life as a fantasist has made him more sceptical. \"Terrorism in itself, I feel the same about. I feel we\\'ve largely invited it. But I grew up in violence; I saw a lot of death and guns when I was growing up, and that kind of thing isn\\'t such a big deal. What I fear are the societal changes that will happen around it, rather than being blown up by anyone. I\\'ll be pissed off if I do get blown up, though, obviously. I won\\'t take kindly to that. At all.\" From Ludmila\\'s Broken English, I get the sense that Finlay has a very idiosyncratic idea of sex. To go back to his twins, the chief respect in which they are opposite is that one has a consuming sex drive and the other has none at all. The one with none is by far the more intelligent, the more clear-sighted, the less confused, but also totally without hope. It is as though sex is this chimera that stands between all of us and despair. Is that really how he sees it? \"I don\\'t know. Probably not. You\\'re only the second or third person I\\'ve heard from that\\'s read it.\" Don\\'t you show it to your friends? \"I don\\'t know anyone who would like it. When I finished Vernon, in the mental journal we all have of peers and friends, I couldn\\'t think of a single person that would like it. Plus, I don\\'t print them till they\\'re finished. I just have them on screen. So nobody can see it.\" You could invite somebody over. \"But who?\" On the matter of relationships, it strikes me as deeply curious, for a person well into adulthood, that he is able to uproot himself to a place where he knows no one, and be content. \"Well, I\\'ve had long relationships. I\\'m not that much of an enigma, really. I\\'m a bit of a loner, but I\\'m very gregarious. I don\\'t have any conviction or opinion that will stand above my ability to chuck them out and have a good yarn and a beer with somebody. Because I\\'ve been so wrong. You have to understand, I\\'ve been shown in my life that I\\'ve been so wrong. Everyone tells you to trust your instincts, and to big yourself up, and go for it, and I did that and I was wrong.\" There\\'s something about his delivery, something distant and playful, that makes you constantly wonder whether he\\'s telling you the truth. I don\\'t know if his penitence is real. I don\\'t even know if it\\'s true that he got into all this debt. His most famous debt, the one that attracted all the attention when he won the Booker, was to the American painter Robert Lenton. Finlay faked a letter in Spanish, which Lenton didn\\'t understand, in order to finagle some money out of him for a property Finlay didn\\'t possess. But the letters Lenton\\'s family subsequently sent to the papers, I wouldn\\'t put it past this Dirty Pierre to have written them himself, and somehow got them posted in Newfoundland as a jape. (Lenton\\'s daughter-in-law wrote, \"Sheathed in his tuxedo of humility and self-awareness, Finlay still refuses to acknowledge what he did\", although Lenton himself said he had no axe to grind and had forgiven him.) Finlay says his third novel is seriously depraved, and he\\'s just trying to finish it before the new atmosphere of moral rectitude starts making demands on the literary establishment - that, I believe. But when he tells me he\\'s writing a children\\'s book with animals in it, I can\\'t ever see that appearing in print, and I couldn\\'t say whether it\\'s because I think he\\'s joking, or lying. I do believe the great big scar on his head came from a car accident, but he says somebody told him it looked like a bullfighting wound, and if he\\'d told me that, I would have believed it, too. \"This is all reconstructed, this side of my face. If you look really close, my eye has been out. The skin is discoloured. It\\'s off my back or my arse or something. I can get people to kiss me there, and they will never know they\\'re kissing my arse.\" He could tell me the Pope had kissed his arse, and I wouldn\\'t know if he meant his regular arse, the arse-section of his face, the regular pope, a person called Pope he\\'d met on his many travels, or whether he was making the whole thing up. Reading his books, it is obvious that there is an honesty and authenticity in the way Finlay writes, and in the way he relates to the world, that renders his reliability when it comes to real events more or less irrelevant. Still, I wouldn\\'t lend him any money.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0444a",
   "metadata": {},
   "source": [
    "There are some issues with the data, so I wrote a quick script to fix it. \n",
    "- Merge train, test and validate as pandas df\n",
    "- Create new Dataset\n",
    "- Do my own train_test_split\n",
    "\n",
    "Most often this is **not** the case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eb57db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154f85df209d4aa2883e4904ef4088f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b822cf41de64b0085763dcdc99faedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/444 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fix_guardian_data(dataset):\n",
    "    # Add a label column to the data\n",
    "    dataset['train'] = dataset['train'].add_column(\"label\", dataset['train']['author'])\n",
    "    dataset['test'] = dataset['test'].add_column(\"label\", dataset['test']['author'])\n",
    "    dataset['validation'] = dataset['validation'].add_column(\"label\", dataset['validation']['author'])\n",
    "    \n",
    "    # We want to do our own test-train split\n",
    "    # To do this, I first make the data into one big dataframe\n",
    "    train_df = pd.DataFrame(dataset['train'])\n",
    "    test_df = pd.DataFrame(dataset['test'])\n",
    "    val_df = pd.DataFrame(dataset['validation'])\n",
    "    all_data = pd.concat([train_df, test_df, val_df])\n",
    "    \n",
    "    # Now I create a Huggingface dataset from that dataframe\n",
    "    dataset = Dataset.from_pandas(all_data)\n",
    "    \n",
    "    # I decide which column is the 'label' column\n",
    "    dataset = dataset.class_encode_column(\"label\")\n",
    "    \n",
    "    # Then I take the train_test_split. I want 20% of the data to be in the test set\n",
    "    dataset = dataset.train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
    "    return dataset\n",
    "\n",
    "dataset = fix_guardian_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "981abe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['author', 'topic', 'article', 'label', '__index_level_0__'],\n",
       "        num_rows: 355\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['author', 'topic', 'article', 'label', '__index_level_0__'],\n",
       "        num_rows: 89\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6702b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(dataset['train']['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bff51f1",
   "metadata": {},
   "source": [
    "Now we tokenize, as always for NLP. \n",
    "- Different LLM's use different tokenizers. \n",
    "- Like the model, our tokenizer needs to know where in the Huggingface Hub to look for specs to tokenize\n",
    "- We can use the  ```AutoTokenizer``` class instead of setting a particular tokenizer class\n",
    "\n",
    "We will be using DistilBERT, a smaller and more nimble version of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b11a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d751c1e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7556cb7ad29f4755996cb39c25e48fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39896dec61245c4bab47cddefacaf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99560bc0b3a144c1be1f3243f0a563d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea5c10fbf514ac7bfafe19f3f3cc7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdd6d3bc4a1427cb04ff26d69ef714e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff40727aaa29423eb8413762c9d66d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/89 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"article\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8564a2d0",
   "metadata": {},
   "source": [
    "Next we set our hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c022bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd64fbc",
   "metadata": {},
   "source": [
    "We feed most hyperparameters to the the [```TrainingArguments``` class](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684a86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\", \n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_rate=1e-5,\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "716c6bf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Now we can specify our model.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Now we can specify our model.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "603627a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35920069d1c44b16b4c4c77da5eb3cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254a6c6",
   "metadata": {},
   "source": [
    "When we train, we want to keep track of the model performance. For this we need to give the model a fucntion that takes in the eval and returns some sort of ... . For this we can use the ```evaluate``` library and write a function around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd056a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3680432c4a6e4a11b76f60e3421bf363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b820e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7853ea",
   "metadata": {},
   "source": [
    "We can also create so called \"callbacks\". \n",
    "- These are objects that customize the training loop\n",
    "- Some of the deftault ones have [their own classes in HuggingFace](https://huggingface.co/docs/transformers/v4.31.0/en/main_classes/callback)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cbb5df",
   "metadata": {},
   "source": [
    "In our case, we want the model to stop if it didn't improve during 3 sequential epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde2f0f",
   "metadata": {},
   "source": [
    "Finally, we create a [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer). \n",
    "- This is a class HuggingFace inherits from [```PyTorch Lightning```](https://lightning.ai/docs/pytorch/stable/common/trainer.html)\n",
    "- Used in many other libraries, like TorchGeo\n",
    "- Given an instance of a model class, this does the whole job of forward and backward passing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44a6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"].shuffle(seed=42),\n",
    "    eval_dataset=tokenized_datasets[\"test\"].shuffle(seed=42),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff999ee",
   "metadata": {},
   "source": [
    "Now we just run ```train()```, like with ```PyTorch```!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb4a00",
   "metadata": {},
   "source": [
    "The model has finished training! \n",
    "- Now we can use it in a ```Huggingface``` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae339184",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a3616",
   "metadata": {},
   "source": [
    "The model outputs probabilities, no need to work with logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b1018",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(tokenized_datasets[\"test\"][50]['article'][:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bcf2fa",
   "metadata": {},
   "source": [
    "Now we'll compare the model predictions on the test set with our predictions on it.\n",
    "- We'll check if ```pred_lab == real_lab``` and count how many times it´s ```True```.\n",
    "    - This is our count for how many times we predicted correctly :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for idx in range(len(tokenized_datasets[\"test\"]['author'])):\n",
    "    # We pull the predicted label from each prediction\n",
    "    # The model is only able to predict based on the 512 first tokens\n",
    "    pred_lab = pipe(tokenized_datasets[\"test\"][idx]['article'][:512])[0]['label']\n",
    "    \n",
    "    # The model outputs strings. \n",
    "    # We pull the number from it and turn it into an integere\n",
    "    pred_lab = int(re.findall(r'\\d+', pred_lab)[0])\n",
    "    \n",
    "    # We get the real label from the test data itself\n",
    "    real_lab = tokenized_datasets[\"test\"][idx]['label']\n",
    "    \n",
    "    # Now we compare and append to a list\n",
    "    correct.append(pred_lab == real_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb76751",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d275e69",
   "metadata": {},
   "source": [
    "We could do more analysis. For example:\n",
    "- Which authors did the model struggle with?\n",
    "- Which did it predict confidently?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a774cc",
   "metadata": {},
   "source": [
    "Further work could include:\n",
    "- Pull out the model weights to see if there are specific words tha are important for predicting specific authors?\n",
    "- Test if we can deceive the model by performing style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8038a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
