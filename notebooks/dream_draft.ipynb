{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd532aad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Here's our N-gram model: what we have so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66071bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b86ba",
   "metadata": {},
   "source": [
    "2. numbers are a problem for n-gram models becayse there are so many of them. we don't want to eliminate them, because they are meaningful, but we want to abstract away from the individual numbers. In addition, we might want to get rid of some other things like parentheticals and quotes, becayse these impossible for our model to keep track of given it's amount of memory. We can take care of these things in the preprocessing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b11df",
   "metadata": {},
   "source": [
    "Here is a final version of our class with all the bells and whistles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e46957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, ELEProbDist\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from functools import reduce\n",
    "\n",
    "class NgramModel():\n",
    "\n",
    "    \n",
    "    def __init__(self, corpus, n):\n",
    "        self.n = n\n",
    "        tokenized_corpus = self._tokenize(corpus)\n",
    "        self._ngrams = self._build_ngrams(tokenized_corpus, n)\n",
    "        self._cpd = self._build_distribution(self._ngrams, n)        \n",
    "\n",
    "    def _tokenize(self, corpus):\n",
    "        # The list of regular expressions and replacements to be applied\n",
    "        # the order here matters! these replacements will happen in order\n",
    "        replacements = [\n",
    "             [\"[-\\n]\",                   \" \"] # Hyphens to whitespace\n",
    "            ,[r'[][(){}#$%\"]',           \"\"] # Strip unwanted characters like quotes and brackets\n",
    "            ,[r'\\s([./-]?\\d+)+[./-]?\\s', \" [NUMBER] \"] # Standardize numbers\n",
    "            ,[r'\\.{3,}',                 \" [ELLIPSIS] \"] # remove ellipsis\n",
    "            ,[r'(\\w)([.,?!;:])',         r'\\1 \\2' ]  # separate punctuation from previous word\n",
    "        ]\n",
    "        \n",
    "        # This is a function that applies a single replacement from the list\n",
    "        resub = lambda words, repls: re.sub(repls[0], repls[1], words)\n",
    "        \n",
    "        # we use the resub function to applea each replacement to the entire corpus,\n",
    "        normalized_corpus = reduce(resub, replacements, corpus)\n",
    "        \n",
    "        \n",
    "        sentences = normalized_corpus.split('.')\n",
    "        \n",
    "        tokens = []\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split() # split on whitespace\n",
    "            words = [word.lower() for word in words]\n",
    "            words = list(pad_both_ends(words, n=self.n))\n",
    "            tokens += words\n",
    "        \n",
    "        return tokens\n",
    "            \n",
    "    def _build_ngrams(self, tokenized_corpus, n):\n",
    "        n_grams = []\n",
    "        for i in range(n-1, len(tokenized_corpus)): \n",
    "            n_grams.append(tuple(tokenized_corpus[i-(n-1):i+1]))    \n",
    "        return n_grams\n",
    "    \n",
    "    def _build_distribution(self, corpus, n):\n",
    "               \n",
    "        cfd = ConditionalFreqDist()\n",
    "        for ngram in self._ngrams:\n",
    "            condition = tuple(ngram[0:n-1]) \n",
    "            outcome = ngram[n-1]\n",
    "            \n",
    "            cfd[condition][outcome] += 1\n",
    "        bins = len(cfd) # we have to pass the number of bins in our freq dist in as a parameter to probability distribution, so we have a bin for every word\n",
    "        cpd = ConditionalProbDist(cfd, ELEProbDist, bins)\n",
    "        self.cpd = cpd\n",
    "        return cpd\n",
    "        \n",
    "    def generate(self, num_sentences = 1, seed = []):\n",
    "        \"\"\"\n",
    "        There are two cases to deal with here. Either we have a start string, or we don't. \n",
    "        If we are given a start string, we'll have to find the last n-1 gram and condition on that\n",
    "        If we are not, we need to generate the first n-1 gram. For a trigram model, we need a bigram. But how can we use our model to generate new words when we have fewer than two words to condition on?\n",
    "        We can use a bigram model! But wait. If we have a bigram model, how do we generate the first token without another token to condition on? \n",
    "        We can use a unigram model! \n",
    "        Recursion will save us here. Turns out the easiest way to do this will be to recursively construct an n-1gram model and store it in the main model.\n",
    "        And how can we \n",
    "        Either way, we need a seed condition to enter into the loop with.\n",
    "        \"\"\"\n",
    "\n",
    "        # place to put generated tokens\n",
    "        string = []\n",
    "\n",
    "        if seed:\n",
    "            string = string + (list(pad_sequence(seed, self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        else:\n",
    "            string = string + (list(pad_sequence('', self.n, pad_left=True, pad_right=False, left_pad_symbol='<s>') ) )\n",
    "        \n",
    "        for i in range(num_sentences):\n",
    "            next_token = tuple(string[-(self.n-1):])\n",
    "            \n",
    "            # keep generating tokens as long as we havent reached the stop sequence\n",
    "            while next_token != '</s>':\n",
    "                \n",
    "                # get the last n-1 tokens to condition on next\n",
    "                lessgram = tuple(string[-(self.n-1):])\n",
    "\n",
    "    \n",
    "                next_token = self.cpd[lessgram].generate()\n",
    "                string.append( next_token )\n",
    "\n",
    "        string = ' '.join(string)\n",
    "        string = add_stops(string)\n",
    "\n",
    "        return string\n",
    "\n",
    "    \n",
    "    def add_stops(string):\n",
    "        \"\"\"\n",
    "        function to convert the stop/start sequence back into periods.\n",
    "        strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "        and replaces them with a period.\n",
    "\n",
    "        then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "        \"\"\"\n",
    "        string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "        string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "        string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf90c92-e6a7-4b97-98bc-c3faccc536d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stops(string):\n",
    "    \"\"\"\n",
    "    function to convert the stop/start sequence back into periods.\n",
    "    strips all the sequences of any number of stop tokens followed by the some number of start tokens\n",
    "    and replaces them with a period.\n",
    "\n",
    "    then strips any remaining stop and start sequences (which will occur at the beginning and end of our entire generated sequence)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"</s>(?:\\s</s>)*\\s<s>(?:\\s<s>)*\", \".\", string)\n",
    "\n",
    "    string = re.sub(r\"(<s>\\s)+\", \"\", string) # initial tokens\n",
    "    string = re.sub(r\"(</s>)\", \"\", string) # final token\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591a9ce",
   "metadata": {},
   "source": [
    "We try generating a 4-gram model with the King James Bible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b20358",
   "metadata": {},
   "source": [
    "Our model expects its training corpus in the form of a single string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad1d6e2",
   "metadata": {},
   "source": [
    "# Let's do a mashup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c1a90",
   "metadata": {},
   "source": [
    "Intro to beautiful soup for scraping web text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23cb8e83-ebff-4192-a0b8-f2c925780b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"dream_journal.txt\"\n",
    "\n",
    "# Read the content of the file and store it in a variable\n",
    "with open(file_path, 'r') as file:\n",
    "    text_dream = file.read()\n",
    "\n",
    "# Now, the 'text_variable' holds the content of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e319cad7-bf60-439c-924a-a6461e088bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = NgramModel(text_dream, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5e36090c-4912-49a0-91b7-f1a40b87bc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"three tuxedo cats now and plays s cascading beautiful song that their friend comes from its huge body compressing me some clothes are in french !** i'm trying to an underground base while ritchie holds my new crush wrote that were three dimensional sweater hands , and hay and said your clothes and i kept my clothes and sounds complicated , like a woman looked at home and monitoring our lawn . set in my clothes are stalking us at a harp and two friends . it's different . they've been placing creepy doll versions of him and junk and i could see some of her life . the drum kit playing the pressure from the performer plucks the next thirty days of material in my shoes \""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86054ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "\n",
    "from bs4 import *\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://theanarchistlibrary.org/library/david-graeber-anarchy-in-a-manner-of-speaking'\n",
    "res = requests.get(url)\n",
    "html_page = res.text\n",
    "\n",
    "# Parse the source code using BeautifulSoup\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "# Extract the plain text content\n",
    "text = soup.get_text()\n",
    "\n",
    "# Print the plain text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbeb3711-c89e-468e-96f1-66530111c9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4\n",
    "\n",
    "from bs4 import *\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "url = 'https://theanarchistlibrary.org/library/laboria-cuboniks-xenofeminism'\n",
    "res = requests.get(url)\n",
    "html_page = res.text\n",
    "\n",
    "# Parse the source code using BeautifulSoup\n",
    "soup = BeautifulSoup(html_page, 'html.parser')\n",
    "\n",
    "# Extract the plain text content\n",
    "text2 = soup.get_text()\n",
    "\n",
    "# Print the plain text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e2e9bf-729a-46e9-90bc-0719551daddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"profane_quotes.txt\"\n",
    "\n",
    "# Read the content of the file and store it in a variable\n",
    "with open(file_path, 'r') as file:\n",
    "    text_profane = file.read()\n",
    "\n",
    "# Now, the 'text_variable' holds the content of the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a408e09d-4c49-44fe-ae1e-71fb8a6d0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"chiang.txt\"\n",
    "\n",
    "# Read the content of the file and store it in a variable\n",
    "with open(file_path, 'r') as file:\n",
    "    text_chiang = file.read()\n",
    "\n",
    "# Now, the 'text_variable' holds the content of the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9f0e008-5f46-49e0-a5c5-525d8e3e8a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"gex.txt\"\n",
    "\n",
    "# Read the content of the file and store it in a variable\n",
    "with open(file_path, 'r') as file:\n",
    "    text_gex = file.read()\n",
    "\n",
    "# Now, the 'text_variable' holds the content of the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c57d0783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3656\n",
      "5330\n",
      "31980\n"
     ]
    }
   ],
   "source": [
    "print(len(text_gex))\n",
    "print(len(text_profane))\n",
    "print(len(text_profane * 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f269771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mashup = text_gex + text_profane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1277f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(mashup, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2a024dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hurry to my eyes this wet it all bullshit ; it's inside spit in yo' shit up 'fore i felt the yard smoking dope , hit list i wear a hook in this house hol' up barracks . in this song you'll just pointless and treated badly . sitting around , fuck me and ask for walks , theoretical discussions that are paid and treated badly . the new mercedes i've been a bucket and made it i was trying to think i let him taste it your hit list i couldn't go i'll never wanted you were talkin' business aiming with a king cobra with your weapon i want you were all work before a shit better off if he cheating put him some' to music , feeling now ? so often do if i wanna gag , go oh whoo ! [number] another distinction that's ok feeling like a weed smoker not enough you getting lazy ? malibu barbie are you , in hollywood , don't wanna party ? clear blue sky but you free’ . they're in never make ya if it cream , we don't clean but i hear you're gone and ask , and you were all the workers who do if i want a bottom feeder big mack truck right in a way similar \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b4a1cac-6e36-49f2-bf83-8a9054e4b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"carly.txt\"\n",
    "\n",
    "# Read the content of the file and store it in a variable\n",
    "with open(file_path, 'r') as file:\n",
    "    text_carly = file.read()\n",
    "\n",
    "# Now, the 'text_variable' holds the content of the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fae70d2-be2b-4057-b115-43ed97ecb0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"duncan.txt\"\n",
    "\n",
    "# Read the content of the file and store it in a variable\n",
    "with open(file_path, 'r') as file:\n",
    "    text_duncan = file.read()\n",
    "\n",
    "# Now, the 'text_variable' holds the content of the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "933f53d3-0d85-473b-ab7c-3d6211d5e4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15332\n",
      "37635\n",
      "30664\n"
     ]
    }
   ],
   "source": [
    "print(len(text_carly))\n",
    "print(len(text_chiang))\n",
    "print(len(text_carly * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d975b8e4-4db5-4d2a-bae9-cc2bcc76367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mashup2 = text_carly * 3 + text_chiang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1945446e-d89d-4d32-bb0e-40f8242ae736",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = NgramModel(mashup2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9118aee4-6142-4bfd-ba3b-ef852f6d5b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what they could power that light passes greenly through , but inherent in my restraints until the duration of gradually increasing magnification , not only some basic arithmetic tests suggested that if the same time , its neighboring universe began to examining flakes of divine the final moments ; even larger in principle no different , install valves that had a slender hope . i replaced the first became widely known , we led . with a desire for the component’s operation will weaken and borrow keep a hundred years before . laboriously , making me just a dozen subassemblies , baby late night i want to those who has a compliment ? it's in secret oh yeah , so out run when we're gonna get lost , you so out make me over tonight over tonight while longer one farther back of filling stations crowded with what made this has dedicated itself was unable to displace at the ever beheld , surrounded by jonathan strahan . many called for a mechanism , sleeping let's take me ? you spoke the excess atmosphere from my brain is calculable , ooh ooh baby ? i got the other should spend years before his installed pair of actuating rods allowed a reservoir below \""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b27e221-cc68-4265-adf4-f06861370225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"omelas.txt\"\n",
    "\n",
    "# Read the content of the file and store it in a variable\n",
    "with open(file_path, 'r') as file:\n",
    "    text_omelas = file.read()\n",
    "\n",
    "# Now, the 'text_variable' holds the content of the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c78b9406-4ffc-4430-97ec-19f3130ec935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"chiang.txt\"\n",
    "\n",
    "# Read the content of the file and store it in a variable\n",
    "with open(file_path, 'r') as file:\n",
    "    text_chiang = file.read()\n",
    "\n",
    "# Now, the 'text_variable' holds the content of the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "293259a2-103b-4da8-a922-a666f287c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15546\n",
      "3656\n",
      "14624\n"
     ]
    }
   ],
   "source": [
    "print(len(text_omelas))\n",
    "print(len(text_gex))\n",
    "print(len(text_gex * 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb3f50c3-5f7a-45ff-860b-d6dd9653d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "mashup3 = text_omelas + text_gex * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de626ac4-a9a1-44eb-923c-747eafdb13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = NgramModel(mashup3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0006fe0a-fb4a-4161-8788-9e4f05d12688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"we do something rather stupid . indeed , others are not know where on they begin to crush it's not naïve and the glory of summer came to praise despair is . for the compression i'll never wanted you see the yard smoking dope , ok doing now but peer in [number] them horrible . theirs is naked . to exchange , fuelless power , piercing \""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363b0b1",
   "metadata": {},
   "source": [
    "Wow!\n",
    "\n",
    "'among the hills that are weaned from the waters saw thee polluted in thy glory above all people , from beersheba to mount up with . [number] : [number] and shaalabbin , and partly broken . report , that jehoshaphat the king is among us still believed in hope ; patient in spirit ; and half of thy power preserve thou those that served in the womb : if jacob take a lump of figs were set there upon him shall inherit all things thereon . in most militants this search for my gold and the lord separated the sons shall eat clean provender , which loveth thee and abishai , and kings have had dominion over our cattle . then he sacrificed also and to whomsoever he will prosper us ; thus have been occupied therein . yellowed figures of cherubims and palm trees : they serve not thy left side , upon their altars : but according to our hand be upon every fowl of the european union . all these did moses command joshua , this do ye look on us ; because a deep sleep fell upon it before saul : [number] wise men , let them turn their mourning . after theo’s rape , a strong wind ? [number] : [number] open thou mine affliction . to him remaining . rather comically , he took counsel how they might attain to innocency ? [number] : [number] for behold the place hormah  '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646db1d",
   "metadata": {},
   "source": [
    "# Exercise / Homework??\n",
    "\n",
    "Make a mashup of two texts. They can be texts you wrote (a collection of tweets, an essay), or from anywhere. You can use libgen to find books and Calibre to convert them to text. Either paste the text directly into a notebook or use a Python utility for reading files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15494939",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpiss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m alice \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mcorpiss\u001b[49m\u001b[38;5;241m.\u001b[39mgutenberg\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcarroll-alice.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      2\u001b[0m now \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(alice))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpiss' is not defined"
     ]
    }
   ],
   "source": [
    "alice = (' ').join(corpiss.gutenberg.words('carroll-alice.txt'))\n",
    "now = text\n",
    "\n",
    "print(len(alice))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1125b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mashups = alice + text\n",
    "model = NgramModel(mashups, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9cef5",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "most used: \n",
    "* https://notebook.community/luketurner/ipython-notebooks/notebooks/n-gram%20tutorial\n",
    "* https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
    "* https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "others:\n",
    "* https://eliteai-coep.medium.com/building-n-gram-language-model-from-scratch-9a5ec206b520\n",
    "* https://github.com/joshualoehr/ngram-language-model/blob/master/language_model.py\n",
    "* http://www.pygaze.org/2016/03/how-to-code-twitter-bot/\n",
    "    - code: https://github.com/esdalmaijer/markovbot\n",
    "* https://towardsdatascience.com/implementing-a-character-level-trigram-language-model-from-scratch-in-python-27ca0e1c3c3f"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
